{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning PyTorch with Examples (1)\n",
    "\n",
    "Codes are identical to: [pytorch tutorial](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warmup: NumPy\n",
    "\n",
    "Before directly trying PyTorch, we will implement simple neural network using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31900382.275380343\n",
      "20 216525.89680977212\n",
      "40 27922.71699749924\n",
      "60 6152.411313553863\n",
      "80 1837.8875162748384\n",
      "100 662.9635080426337\n",
      "120 267.0818121612483\n",
      "140 114.65948277077416\n",
      "160 51.146208705379266\n",
      "180 23.382495645755704\n",
      "200 10.868462006957207\n",
      "220 5.111242712521417\n",
      "240 2.4243507134483684\n",
      "260 1.157142111453086\n",
      "280 0.5549743741841484\n",
      "300 0.2671512299468225\n",
      "320 0.128972451138516\n",
      "340 0.06240927262403543\n",
      "360 0.030257205879535523\n",
      "380 0.014692555163470617\n",
      "400 0.007143893346126569\n",
      "420 0.003477607268483101\n",
      "440 0.0016945763002369043\n",
      "460 0.0008264629315300131\n",
      "480 0.000403398652010207\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 64\n",
    "D_in, H, D_out = 1000, 100, 10\n",
    "\n",
    "# input, output\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward propagation\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 20 == 0: print(t, loss)\n",
    "    \n",
    "    # backward propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # (H, N) dot (N, D_out) => (H, D_out)\n",
    "\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # (N, D_out) dot (D_out, H) => (N, H)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    \n",
    "    grad_w1 = x.T.dot(grad_h) # (D_in, N) dot (N, H) => (D_in, H)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Tensors\n",
    "\n",
    "Why PyTorch: use concept **Tensor**, that can utilize GPU on its computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33310936.0\n",
      "20 241361.09375\n",
      "40 29715.150390625\n",
      "60 5819.16015625\n",
      "80 1404.984375\n",
      "100 379.70257568359375\n",
      "120 109.82647705078125\n",
      "140 33.211524963378906\n",
      "160 10.366438865661621\n",
      "180 3.313112735748291\n",
      "200 1.078428030014038\n",
      "220 0.35614708065986633\n",
      "240 0.11905832588672638\n",
      "260 0.040213409811258316\n",
      "280 0.013779652304947376\n",
      "300 0.004883614834398031\n",
      "320 0.001871271408163011\n",
      "340 0.0008093949290923774\n",
      "360 0.0003989505930803716\n",
      "380 0.00022095948224887252\n",
      "400 0.0001338231231784448\n",
      "420 8.757281466387212e-05\n",
      "440 6.118955207057297e-05\n",
      "460 4.4693417294183746e-05\n",
      "480 3.455495243542828e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N = 64\n",
    "D_in, H, D_out = 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # forward propagation\n",
    "    # mm: matrix multiplication\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # item() works only on one element tensor\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 20 == 0: print(t, loss)\n",
    "    \n",
    "    # backward propagation\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    \n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    \n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "\n",
    "- PyTorch autograd package supports auto computation of backward passes.\n",
    "- If `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another tensor that holds the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36565664.0\n",
      "20 221575.828125\n",
      "40 27674.568359375\n",
      "60 5455.25390625\n",
      "80 1302.4947509765625\n",
      "100 341.1291198730469\n",
      "120 94.07901000976562\n",
      "140 26.78451919555664\n",
      "160 7.795941352844238\n",
      "180 2.3056395053863525\n",
      "200 0.6908807754516602\n",
      "220 0.20928654074668884\n",
      "240 0.06400889158248901\n",
      "260 0.0198016706854105\n",
      "280 0.0062724994495511055\n",
      "300 0.002143196063116193\n",
      "320 0.000842496519908309\n",
      "340 0.00038975474308244884\n",
      "360 0.0002074890653602779\n",
      "380 0.00012354753562249243\n",
      "400 8.00056877778843e-05\n",
      "420 5.549774505198002e-05\n",
      "440 4.073899981449358e-05\n",
      "460 3.123254646197893e-05\n",
      "480 2.468096135999076e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N = 64\n",
    "D_in, H, D_out = 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 20 == 0: print(t, loss.item())\n",
    "    \n",
    "    # auto backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # pause tracking of autograd\n",
    "    # update w1, w2 and zero the grads\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining new autograd functions\n",
    "\n",
    "Primitive autograd operator is two functions that operate on Tensors.\n",
    "- `forward()`: computes output tensors from input tensors.\n",
    "- `backward()`: computes gradients of the input tensors. (by receiving gradients of output)\n",
    "\n",
    "We define our own autograd operator by defining a subclass of `torch.autograd.Function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34426288.0\n",
      "20 228508.71875\n",
      "40 33925.78515625\n",
      "60 8762.6279296875\n",
      "80 2869.01025390625\n",
      "100 1067.605224609375\n",
      "120 428.014892578125\n",
      "140 179.7257843017578\n",
      "160 77.81365966796875\n",
      "180 34.41069030761719\n",
      "200 15.454386711120605\n",
      "220 7.022711753845215\n",
      "240 3.2210559844970703\n",
      "260 1.4885547161102295\n",
      "280 0.6921794414520264\n",
      "300 0.3236069679260254\n",
      "320 0.15201005339622498\n",
      "340 0.07172486931085587\n",
      "360 0.034040845930576324\n",
      "380 0.016309797763824463\n",
      "400 0.00796152651309967\n",
      "420 0.004013003781437874\n",
      "440 0.0021224436350166798\n",
      "460 0.0011807273840531707\n",
      "480 0.0006944339256733656\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_):\n",
    "        ctx.save_for_backward(input_)\n",
    "        return input_.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input_, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input_ < 0] = 0\n",
    "        return grad_input\n",
    "    \n",
    "dtype = torch.float\n",
    "device = torch.device('cpu')\n",
    "\n",
    "N = 64\n",
    "D_in, H, D_out = 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 20 == 0: print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
